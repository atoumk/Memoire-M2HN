{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2061368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b4a25",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5543af",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = str(Path.cwd().resolve().parents[1]/ 'src')\n",
    "sys.path.append(src)\n",
    "from config.paths import ROOT, DATA, EXPERIMENTS, METADATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc58e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "listes_dir = os.path.join(METADATA, 'acquisition', 'encheres_listes')\n",
    "logs_dir = os.path.join(METADATA, 'acquisition', 'logs')\n",
    "\n",
    "json_path = os.path.join(METADATA, 'catalogues', 'raw', 'catalogue_encheres.json')\n",
    "output_dir = os.path.join(DATA, 'raw_collected', 'objets', 'encheres')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ffe8e",
   "metadata": {},
   "source": [
    "# Fonctions parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a955a3c",
   "metadata": {},
   "source": [
    "## Bonhams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d702f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonhams_parser(soup) : \n",
    "    script_tag = soup.find('script', id='__NEXT_DATA__')\n",
    "    json_text = script_tag.string\n",
    "    data = json.loads(json_text)\n",
    "    pageProps = (data.get(\"props\", {})\n",
    "             .get(\"pageProps\", {})\n",
    "            )\n",
    "\n",
    "    pageProps = (data.get(\"props\", {})\n",
    "                .get(\"pageProps\", {})\n",
    "            )\n",
    "    lot = (pageProps.get('lot'))\n",
    "    lot_nb = (lot.get('iSaleLotNo'))\n",
    "    \n",
    "    # Description\n",
    "    sStyledDesc = pageProps.get('sStyledDesc')\n",
    "    if not sStyledDesc:\n",
    "        sStyledDesc = lot.get('sStyledDesc')\n",
    "    sCatalogDesc = pageProps.get('sCatalogDesc')\n",
    "\n",
    "    desc = None\n",
    "\n",
    "    if sStyledDesc:\n",
    "        soup = BeautifulSoup(sStyledDesc, 'html.parser')\n",
    "        first = soup.find('div', class_='firstLine')\n",
    "        second = soup.find('div', class_='secondLine')\n",
    "        parts = []\n",
    "        if first: parts.append(first.get_text(strip=True))\n",
    "        if second: parts.append(second.get_text(strip=True))\n",
    "        desc = ', '.join(parts)\n",
    "\n",
    "    elif sCatalogDesc:\n",
    "        soup = BeautifulSoup(sCatalogDesc, 'html.parser')\n",
    "        lot_name_div = soup.find('div', class_='LotName')\n",
    "        if lot_name_div:\n",
    "            # Replace <br> with comma + space, then strip\n",
    "            text = lot_name_div.get_text(separator=', ', strip=True)\n",
    "            desc = text\n",
    "\n",
    "    auction_dict = pageProps.get('auction', {})\n",
    "    dates = auction_dict.get('dates', {})\n",
    "    datetime_str = dates.get('end').get('datetime')\n",
    "    date_iso = datetime_str.split('T')[0]\n",
    "    date_obj = datetime.strptime(date_iso, \"%Y-%m-%d\")\n",
    "    date_str = date_obj.strftime(\"%d %B %Y\")\n",
    "\n",
    "\n",
    "    vente = auction_dict.get('sSaleName')\n",
    "\n",
    "    image_dicts = pageProps.get('lot').get('images')\n",
    "    image_urls = [img['image_url'] for img in image_dicts if 'image_url' in img]\n",
    "\n",
    "    locations = auction_dict.get('locations')\n",
    "    location = locations[0].get('sAddrs2')\n",
    "\n",
    "    if any(char.isdigit() for char in location):\n",
    "        location_split = location.strip().split()\n",
    "        if len(location_split) >= 3:\n",
    "            location = ' '.join(location_split[:-2])\n",
    "        else:\n",
    "            # Not enough parts to safely remove, so leave unchanged\n",
    "            location = ' '.join(location_split)\n",
    "    else:\n",
    "        # No digits: keep the full string\n",
    "        location = location.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'Description': desc,\n",
    "        'Vente': vente,\n",
    "        'Loc': location,\n",
    "        'Date_str': date_str,\n",
    "        'Date_iso':date_iso,\n",
    "        'Lot': lot_nb,\n",
    "        'Img_urls': image_urls\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3174a65",
   "metadata": {},
   "source": [
    "## Sotheby's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sothebys_parser(soup) :\n",
    "     \n",
    "        script_tag = soup.find('script', id='__NEXT_DATA__')\n",
    "        desc = vente = location = date_iso = date_str = lot_num = None\n",
    "        image_urls = []\n",
    "\n",
    "        if script_tag : \n",
    "                json_text = script_tag.string\n",
    "                json_data = json.loads(json_text)\n",
    "                data = (json_data.get(\"props\", {})\n",
    "                        .get(\"pageProps\", {})\n",
    "                        .get(\"apolloCache\", {}))\n",
    "                \n",
    "                #Trouver dico données de la vente\n",
    "                auction_key = next((key for key in data if key.startswith(\"Auction:\")), None)\n",
    "                data_auction = data.get(auction_key, {}) if auction_key else {}\n",
    "\n",
    "                #dates = data_auction.get('dates', {})\n",
    "                #date_iso = dates['closed'].split('T')[0]\n",
    "\n",
    "                locationV2 = data_auction.get('locationV2', {})\n",
    "                location = locationV2.get('name')\n",
    "\n",
    "                vente = data_auction.get('title')\n",
    "\n",
    "                #Trouver dico données du lot\n",
    "                lot2_key = next((key for key in data if key.startswith(\"LotV2:\")), None)\n",
    "                data_lot = data.get(lot2_key, {}) if lot2_key else {}\n",
    "\n",
    "\n",
    "                desc = data_lot.get('title')\n",
    "                lot_num = data_lot.get('lotNumber', {}).get(\"lotNumber\") \n",
    "                date = data_lot.get('session', {}).get('scheduledOpeningDate')\n",
    "                date_iso = date.split('T')[0]\n",
    "                date_obj = datetime.strptime(date_iso, \"%Y-%m-%d\")\n",
    "                date_str = date_obj.strftime(\"%d %B %Y\")\n",
    "                size_priority = [\"ExtraExtraLarge\", \"ExtraLarge\", \"Large\", \"Medium\", \"Small\"]\n",
    "\n",
    "\n",
    "                media_data = data_lot.get('media({\"imageSizes\":[\"Small\",\"Medium\",\"Large\",\"ExtraLarge\",\"ExtraExtraLarge\"]})', {})\n",
    "\n",
    "                for image in media_data.get('images', []):\n",
    "                        renditions = image.get('renditions', [])\n",
    "                \n",
    "                # Create a dict mapping size -> url\n",
    "                        size_to_url = {r['imageSize']: r['url'] for r in renditions if 'imageSize' in r and 'url' in r}\n",
    "                \n",
    "                # Pick the best available size\n",
    "                        for size in size_priority:\n",
    "                                if size in size_to_url:\n",
    "                                        image_urls.append(size_to_url[size])\n",
    "                                        break  # Stop at the first (largest available) match\n",
    "        else:\n",
    "                desc = soup.find('h1', class_=\"LotPage-productTitle\").string\n",
    "                vente = soup.find('a', class_=\"Link\").string \n",
    "                lot = soup.find('span', class_=\"Link\").string\n",
    "                lot_num = lot.split(' ')[1]\n",
    "\n",
    "                link_tag = soup.find('a', class_='Link')\n",
    "\n",
    "                if link_tag:\n",
    "                        auction_link = link_tag.get('href')\n",
    "                        auction_soup = get_soup(auction_link)  # use the new soup here!\n",
    "\n",
    "                        target_string = auction_soup.find(string=lambda text: text and '•' in text)\n",
    "                        if target_string:\n",
    "                                target_text = target_string.find_parent('div').get_text(strip=True)\n",
    "                                date_str = target_text.split('•')[0].strip()\n",
    "                                location = target_text.split('•')[1].strip()\n",
    "\n",
    "                                date_obj = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "                                date_iso = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                        \n",
    "                img_tag = soup_5.find('img', {'alt': '', 'class': 'Image lazyload'})\n",
    "\n",
    "                if img_tag and img_tag.has_attr('data-srcset'):\n",
    "                        image_urls = [img_tag['data-srcset'].split(' ')[0]]\n",
    "                        time.sleep(10)\n",
    "                        \n",
    "        return {\n",
    "        'Description': desc,\n",
    "        'Vente': vente,\n",
    "        'Loc': location,\n",
    "        'Date_iso': date_iso,\n",
    "        'Date_str' : date_str,\n",
    "        'Lot': lot_num,\n",
    "        'Img_urls': image_urls\n",
    "         }\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c407b76",
   "metadata": {},
   "source": [
    "## Christie's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_chr_components(soup: BeautifulSoup, key=None) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the `window.chrComponents` JavaScript object from a BeautifulSoup-parsed HTML document.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The parsed HTML document.\n",
    "\n",
    "    Returns:\n",
    "        dict: The extracted chrComponents object as a Python dictionary.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the chrComponents object cannot be found or parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    if key : \n",
    "        results = soup.find_all('script')\n",
    "\n",
    "        for script in results : \n",
    "            if key in script.text :\n",
    "                break\n",
    "\n",
    "\n",
    "        # The full JavaScript content inside the script tag\n",
    "        script_text = script.string\n",
    "\n",
    "        # Regex to extract the JS object assigned to window.chrComponents.lotHeader_<ID>\n",
    "        pattern = re.compile(\n",
    "            rf'window\\.chrComponents\\.lotHeader_{key}\\s*=\\s*(\\{{.*?\\}});',\n",
    "            re.DOTALL\n",
    "        )\n",
    "\n",
    "        match = pattern.search(script_text)\n",
    "\n",
    "        if match:\n",
    "            js_object = match.group(1)\n",
    "            \n",
    "            # Try parsing as JSON\n",
    "            try:\n",
    "                data = json.loads(js_object)\n",
    "                return data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Failed to decode JSON. Error:\", e)\n",
    "                data = None\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "            data = None\n",
    "            return data\n",
    "\n",
    "\n",
    "    else : \n",
    "    # Find all <script> tags\n",
    "        scripts = soup.find_all(\"script\")\n",
    "\n",
    "        for script in scripts:\n",
    "            if not script.string:\n",
    "                continue  # Skip if script has no content\n",
    "\n",
    "            # Look for the line that contains window.chrComponents\n",
    "            match = re.search(r'window\\.chrComponents\\s*=\\s*(\\{.*?\\});', script.string, re.DOTALL)\n",
    "\n",
    "            if match:\n",
    "                js_object_str = match.group(1)\n",
    "                try:\n",
    "                    # Try parsing as JSON\n",
    "                    data = json.loads(js_object_str)\n",
    "                    return data\n",
    "                except json.JSONDecodeError as e:\n",
    "                    raise ValueError(\"Found window.chrComponents, but failed to parse JSON\") from e\n",
    "\n",
    "        raise ValueError(\"window.chrComponents not found in the soup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def soup_parse_christies(url) :\n",
    "        soup=get_soup(url)\n",
    "        lot_header = soup.find(\"chr-lot-header\")\n",
    "\n",
    "        if 'online' in url : \n",
    "\n",
    "                lot_num = lot_header.get('lot_id_txt')\n",
    "                vente = lot_header.get(\"sale_title\")\n",
    "\n",
    "                # Get auction date & location using auction url\n",
    "                sale_url = lot_header.get(\"sale_url\")\n",
    "\n",
    "                url_split = url.split('/')\n",
    "\n",
    "                url_split = url.split('/')\n",
    "                sale_nb = sale_url.split('/')[-1].split('?')[0]\n",
    "                sale_url_full = '/'.join(url_split[:5] + [sale_nb])\n",
    "\n",
    "                soup_auction = get_soup(sale_url_full)\n",
    "                auction_components = extract_chr_components(soup_auction)\n",
    "                location = auction_components.get('auction', {}).get('data', {}).get('sale_location')\n",
    "                date = auction_components.get('auction', {}).get('data', {}).get('auction_status', {}).get('sale_end_date_time', None)\n",
    "\n",
    "\n",
    "                # Get desc, img urls\n",
    "                \n",
    "                dico_components = extract_chr_components(soup)\n",
    "\n",
    "                list_img_url = []\n",
    "                lots1 = dico_components.get('lots')\n",
    "                \n",
    "                data = lots1.get('data')\n",
    "                lots = data.get('lots', [])[0]\n",
    "                list_img = lots.get(\"lot_assets\",[])\n",
    "\n",
    "                for item in list_img :    \n",
    "                        img_url = item.get(\"image_url\")\n",
    "                        if img_url : \n",
    "                                clean_url = img_url.split('?')[0]\n",
    "                                list_img_url.append(clean_url)\n",
    "\n",
    "                desc1 = lots.get('title_primary_txt')\n",
    "                desc2 = lots.get( 'title_secondary_txt')\n",
    "                desc = f'{desc1}; {desc2}'\n",
    "\n",
    "        else : \n",
    "                desc1 = lot_header.get('title_primary_txt', '').strip()\n",
    "                desc2 = lot_header.get('title_secondary_txt', '').strip()\n",
    "                namespace = lot_header.get('data-namespace', '').strip()\n",
    "                key = namespace.split('_')[1]\n",
    "                components = extract_chr_components(soup, key)\n",
    "                \n",
    "                date = components.get('data', {}).get('sale', {}).get('end_date')\n",
    "                vente = components.get('data', {}).get('sale', {}).get('title_txt')\n",
    "                location = components.get('data', {}).get('sale', {}).get('location_txt')\n",
    "\n",
    "                list_img_url = []\n",
    "                lots=components.get('data', {}).get('lots',[])[0]\n",
    "                list_img = lots.get(\"lot_assets\",[])\n",
    "\n",
    "                for item in list_img :    \n",
    "                        img_url = item.get(\"image_url\")\n",
    "                        if img_url : \n",
    "                                clean_url = img_url.split('?')[0]\n",
    "                                list_img_url.append(clean_url)\n",
    "\n",
    "                lot_num = lots.get('lot_id_txt')\n",
    "                # Si desc n'est pas extraite avant : \n",
    "                desc1 = desc1 or lots.get('title_primary_txt')\n",
    "                desc2 = desc2 or lots.get('title_secondary_txt')\n",
    "                desc = f'{desc1}; {desc2}'\n",
    "\n",
    "        if date : \n",
    "                date_iso = date.split('T')[0]\n",
    "                date_obj = datetime.strptime(date_iso, \"%Y-%m-%d\")\n",
    "                date_str = date_obj.strftime(\"%d %B %Y\")       \n",
    "\n",
    "                \n",
    "        return {\n",
    "        'Description': desc,\n",
    "        'Vente': vente,\n",
    "        'Loc': location,\n",
    "        'Date_iso': date_iso,\n",
    "        'Date_str' : date_str,\n",
    "        'Lot': lot_num,\n",
    "        'Img_urls': list_img_url\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932fed2c",
   "metadata": {},
   "source": [
    "# Fonctions scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img (url_img, output_dir, object_id) :\n",
    "\n",
    "    folder_name = os.path.basename(output_dir)\n",
    "    img_data = requests.get(url_img).content\n",
    "    img_dir = 'img'\n",
    "    img_dir_path = os.path.join(output_dir, img_dir)\n",
    "    os.makedirs(img_dir_path, exist_ok=True) \n",
    "\n",
    "    output_file = os.path.join(img_dir_path, f\"{folder_name}_{object_id}.jpg\")    \n",
    "    with open(output_file, 'wb') as f : \n",
    "        f.write(img_data)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(lien) :\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36\"\n",
    "    }\n",
    "    result = requests.get(lien, headers=headers)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ae862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(img_tuples):\n",
    "    for img_url, img_path in img_tuples:\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "    print(f'{len(img_tuples)} images saved')\n",
    "    time.sleep(random.uniform(5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75937119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path, metadata_dict: dict, overwrite: bool = False) -> None:\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Catalogue JSON doit être une liste d'entrées\")\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        data = []\n",
    "\n",
    "    new_id = metadata_dict.get('Id')\n",
    "\n",
    "    # Look for existing entry with the same ID\n",
    "    existing_index = next((i for i, entry in enumerate(data) if entry.get('Id') == new_id), None)\n",
    "\n",
    "    if existing_index is not None:\n",
    "        if overwrite:\n",
    "            data[existing_index] = metadata_dict\n",
    "            print(f\"L'entrée avec l'ID '{new_id}' a été mise à jour dans le catalogue.\")\n",
    "        else:\n",
    "            print(f\"L'objet avec l'ID '{new_id}' est déjà dans le catalogue. Aucune modification apportée.\")\n",
    "            return\n",
    "    else:\n",
    "        data.append(metadata_dict)\n",
    "\n",
    "    # Save back to JSON\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1995287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_from_urls(img_urls, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for img_url in img_urls:\n",
    "        # Extract the image filename from the URL\n",
    "        filename = os.path.basename(urlparse(img_url).path)\n",
    "        if not filename:\n",
    "            filename = f'image_{int(time.time() * 1000)}.jpg'  # fallback name\n",
    "        \n",
    "        img_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Download and save the image\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "        print(f'Saved: {img_path}')\n",
    "        \n",
    "        # Optional delay to avoid overwhelming servers\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    print(f'{len(img_urls)} images saved to {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, auteur,maison_encheres, parser_function, output_dir, overwrite, soup=True, json_path=json_path):\n",
    "\n",
    "    if soup : \n",
    "        soup_obj = get_soup(url)\n",
    "        dico_metadata = parser_function(soup_obj)\n",
    "    else :\n",
    "        dico_metadata=parser_function(url)    \n",
    "\n",
    "    lot_id = dico_metadata.get('Lot_id') or f\"{maison_encheres}_{dico_metadata['Date_iso']}_{dico_metadata['Lot']}\"\n",
    "\n",
    "\n",
    "    dico_metadata.update({\n",
    "    'Auteur': auteur,\n",
    "    'Url_lot': url,\n",
    "    'Maison_encheres': maison_encheres,\n",
    "    'Id': lot_id\n",
    "    })\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    img_tuples = []\n",
    "    filenames = []\n",
    "    image_urls = dico_metadata['Img_urls']\n",
    "    \n",
    "    for i, img_url in enumerate(image_urls) :\n",
    "        filename = f\"{dico_metadata['Id']}-{i+1:02d}.jpg\"\n",
    "        filenames.append(filename)\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        img_tuples.append((img_url, filepath))\n",
    "\n",
    "    dico_metadata['Fichiers_img']=filenames\n",
    "    \n",
    "    save_images(img_tuples)\n",
    "    \n",
    "    load_json(dico_metadata, overwrite=overwrite, json_path=json_path)\n",
    "\n",
    "    return dico_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd13f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "def process_lots(url_list, parametres, output_dir, overwrite=False, soup=True):\n",
    "    \"\"\"\n",
    "    Applies main() to a list of URLs using shared metadata provided as a tuple.\n",
    "\n",
    "    Parameters:\n",
    "        url_list: List of URLs (str)\n",
    "        shared_info: Tuple -> (auteur, maison_encheres, parser_function)\n",
    "        output_dir: Directory to save images and metadata\n",
    "        overwrite: Whether to overwrite existing files\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame containing the metadata for all processed lots.\n",
    "    \"\"\"\n",
    "    maison_encheres, parser_function, auteur = parametres\n",
    "    all_metadata = []\n",
    "\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            metadata = main(\n",
    "                url=url,\n",
    "                auteur=auteur,\n",
    "                maison_encheres=maison_encheres,\n",
    "                parser_function=parser_function,\n",
    "                output_dir=output_dir,\n",
    "                overwrite=overwrite,\n",
    "                soup=soup\n",
    "            )\n",
    "            metadata['error'] = None  # No error for this row\n",
    "            all_metadata.append(metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            all_metadata.append({\n",
    "                'url': url,\n",
    "                'error': str(e)\n",
    "                # You can add more default values here if needed, e.g., 'auteur': auteur\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_metadata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457fd3f2",
   "metadata": {},
   "source": [
    "# Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c64bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_encheres (keyword, path_list_dir) : \n",
    "    listes_filenames = os.listdir(path_list_dir)\n",
    "    return [os.path.join(path_list_dir, item) for item in listes_filenames if item.endswith('.txt') and keyword in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sothebys_list = list_encheres('Sothebys', listes_dir)\n",
    "bonhams_list = list_encheres('Bonhams', listes_dir)\n",
    "christies_list = list_encheres('Christies', listes_dir)\n",
    "drouot_list = list_encheres('GazetteDrouot', listes_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67922273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/Sothebys_DeMorgan.txt', '/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/Sothebys_Vieillard.txt']\n",
      "['/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/Bonhams_Longwy.txt', '/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/Bonhams_DeMorgan.txt']\n",
      "['/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/Christies_Vieillard.txt', '/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/Christies_Doulton.txt']\n",
      "['/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/GazetteDrouot_Deck.txt', '/Users/enki/data/Git/Memoire-M2HN/metadata/acquisition/encheres_listes/GazetteDrouot_Lachenal.txt']\n"
     ]
    }
   ],
   "source": [
    "print(sothebys_list[:2])\n",
    "print(bonhams_list[:2])\n",
    "print(christies_list[:2])\n",
    "print(drouot_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(encheres_list, parser, log_dir) : \n",
    "    df_final_list = []\n",
    "\n",
    "    for path in encheres_list :\n",
    "        part_filename = path.split('_')[-1][:-4]\n",
    "        auteur = ' '.join(part_filename.split('-')) if '-' in part_filename else part_filename\n",
    "        maison_encheres = path.split('/')[-1].split('_')[0]\n",
    "        parametres = (maison_encheres, parser, auteur)\n",
    "\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            liste_urls = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        df_auteur = process_lots(liste_urls, parametres, output_dir, overwrite=True)\n",
    "        df_final_list.append(df_auteur)\n",
    "\n",
    "    df_final = pd.concat(df_final_list, ignore_index=True)\n",
    "    csv_file = f'log_acquisition_{maison_encheres}.csv'\n",
    "    df_final.to_csv(os.path.join(log_dir, csv_file), encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b45af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_list(sothebys_list, sothebys_parser, logs_dir)\n",
    "process_list(bonhams_list, bonhams_parser, logs_dir)\n",
    "process_list(sothebys_list, sothebys_parser, logs_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
